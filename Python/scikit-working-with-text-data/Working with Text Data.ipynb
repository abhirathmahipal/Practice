{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.datasets.base.Bunch'>\n",
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "2257\n",
      "2257\n",
      "2257\n",
      "/home/drive/scikit_learn_data/20news_home/20news-bydate-train/comp.graphics/38479\n",
      "From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\n",
      "Subject: help: Splitting a trimming region along a mesh \n",
      "Organization: University Of Kentucky, Dept. of Math Sciences\n",
      "Lines: 28\n",
      "\n",
      "\n",
      "\n",
      "\tHi,\n",
      "\n",
      "\tI have a problem, I hope some of the 'gurus' can help me solve.\n",
      "\n",
      "\tBackground of the problem:\n",
      "\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \n",
      "\tmapping of a 3d Bezier patch into 2d. The area in this domain\n",
      "\twhich is inside a trimming loop had to be rendered. The trimming\n",
      "\tloop is a set of 2d Bezier curve segments.\n",
      "\tFor the sake of notation: the mesh is made up of cells.\n",
      "\n",
      "\tMy problem is this :\n",
      "\tThe trimming area has to be split up into individual smaller\n",
      "\tcells bounded by the trimming curve segments. If a cell\n",
      "\tis wholly inside the area...then it is output as a whole ,\n",
      "\telse it is trivially rejected. \n",
      "\n",
      "\tDoes any body know how thiss can be done, or is there any algo. \n",
      "\tsomewhere for doing this.\n",
      "\n",
      "\tAny help would be appreciated.\n",
      "\n",
      "\tThanks, \n",
      "\tAni.\n",
      "-- \n",
      "To get irritated is human, to stay cool, divine.\n",
      "\n",
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "               categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# print twenty_train\n",
    "print type(twenty_train)\n",
    "print twenty_train.target_names\n",
    "print len(twenty_train.data)\n",
    "print len(twenty_train.target)\n",
    "print len(twenty_train.filenames)\n",
    "print twenty_train.filenames[1]\n",
    "print twenty_train.data[1]\n",
    "print twenty_train.target_names[twenty_train.target[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 35788)\n",
      "[1 1 1 ..., 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "print X_train_counts.shape\n",
    "print (X_train_counts.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 35788)\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print X_train_tf.shape\n",
    "print X_train_tf[1].getnnz() # getting the length of the 1st example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 35788)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is Love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n",
      "'geometry, tranformations' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n",
    "\n",
    "docs_test = ['God is Love', 'OpenGL on the GPU is fast', 'geometry, tranformations']\n",
    "X_test_count = count_vect.transform(docs_test)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_test_count)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_test, predicted):\n",
    "    print '%r => %s' % (doc, twenty_train.target_names[category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        st...False,\n",
       "         use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83488681757656458"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "    categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9127829560585885"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, random_state=42,\n",
    "                                           n_iter=5)),\n",
    "])\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)  \n",
    "\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soc.religion.christian\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "comp.graphics\n",
      "comp.graphics\n"
     ]
    }
   ],
   "source": [
    "print twenty_train.target_names[gs_clf.predict(['God is love'])[0]]\n",
    "print twenty_train.target_names[gs_clf.predict(['geometry'])[0]]\n",
    "print twenty_train.target_names[gs_clf.predict(['jesus geometry'])[0]]\n",
    "print twenty_train.target_names[gs_clf.predict(['jesus geometry computers linear algebra'])[0]]\n",
    "print twenty_train.target_names[gs_clf.predict(['doctors who save lives are no less than gods'])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "print gs_clf.best_score_                                  \n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import site\n",
    "site.getsitepackages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data count 1502\n",
      "0.88948069241\n"
     ]
    }
   ],
   "source": [
    "### Useful tips and a touch of NLTK\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),])\n",
    "\n",
    "print \"Test data count\", len(twenty_test.data)\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "print np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('vect', StemmedCountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "            dtype=<type 'numpy.int64'>, encoding=u'utf-8',\n",
      "            input=u'content', lowercase=True, max_df=1.0,\n",
      "            max_features=None, min_df=1, ngram_range=(1, 1),\n",
      "            preprocessor=No...alse,\n",
      "         use_idf=True)), ('mnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=False))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.89747003994673769"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyser(self):\n",
    "        analyser = super(StemmedCountVectorizer, self).build_analyser()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "\n",
    "text_snowball_stemmed = Pipeline([('vect', stemmed_count_vect),\n",
    "                                  ('tfidf', TfidfTransformer()),\n",
    "                                  ('mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "print text_snowball_stemmed\n",
    "text_snowball_stemmed.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_snowball_stemmed.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35482"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(stemmed_count_vect)\n",
    "len(stemmed_count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
